# ğŸš€ Guia de Deploy no Render

## âœ… O QUE VOCÃŠ TEM

Tudo jÃ¡ estÃ¡ organizado e pronto para deploy:

```
projeto-completo/
â”œâ”€â”€ backend/          (API Flask - pronta!)
â”œâ”€â”€ ollama/           (ServiÃ§o Ollama - pronto!)
â”œâ”€â”€ frontend/         (Website - pronto!)
â”œâ”€â”€ docs/             (Esta documentaÃ§Ã£o)
â”œâ”€â”€ render.yaml       (ConfiguraÃ§Ã£o Render)
â””â”€â”€ README.md
```

---

## ğŸ“‹ PASSO 1: Preparar GitHub

1. **Crie um repositÃ³rio vazio** em github.com
   - Nome: `ollama-image-api`
   - Deixe vazio (nÃ£o inicie com README)

2. **No seu terminal:**

```bash
# Clonar ou iniciar repo
cd seu-repositorio
git init

# Adicionar todos os arquivos
git add .

# Commit inicial
git commit -m "Initial commit: Ollama Image Generation API"

# Configurar branch main
git branch -M main

# Adicionar remote GitHub
git remote add origin https://github.com/SEU-USUARIO/ollama-image-api.git

# Push
git push -u origin main
```

---

## ğŸ¯ PASSO 2: Deploy no Render

### OpÃ§Ã£o A: Usando Blueprint (RECOMENDADO)

1. **VÃ¡ em https://render.com**
2. **FaÃ§a login** (ou crie conta)
3. **Clique em "New +" â†’ "Blueprint"**
4. **Conecte seu repositÃ³rio GitHub**
5. **Selecione `ollama-image-api`**
6. **Clique em "Deploy"**

**Pronto!** Render vai:
- Ler o arquivo `render.yaml`
- Criar automaticamente 2 serviÃ§os:
  - `ollama-image-api` (API Flask)
  - `ollama-server` (ServiÃ§o Ollama)
- Fazer build dos Dockerfiles
- Iniciar tudo automaticamente

---

## â³ O QUE ACONTECE DURANTE O DEPLOY

**Tempo estimado: 45-60 minutos**

### Minutos 1-10: Deploy da API
- Render cria o serviÃ§o `ollama-image-api`
- Compila o Dockerfile (backend/)
- Inicia a API Flask

### Minutos 10-45: Deploy do Ollama
- Render cria o serviÃ§o `ollama-server`
- Baixa imagem Ollama (~2GB)
- Inicializa Ollama
- **AGUARDA: Baixa modelo FLUX (~5GB)** â† Leva tempo!

### Minutos 45-60: Tudo pronto
- API conecta ao Ollama
- Health check passa
- ServiÃ§os prontos para uso

---

## âœ… VERIFICAR SE ESTÃ FUNCIONANDO

### 1. Ver Logs do Render

1. VÃ¡ em https://render.com â†’ Dashboard
2. Clique no serviÃ§o `ollama-image-api`
3. VÃ¡ em **"Logs"**
4. Procure por mensagens de sucesso

### 2. Testar Health Check

```bash
curl https://bhjjii.onrender.com/health
```

**Resposta esperada:**
```json
{
  "status": "ok",
  "ollama_connected": true,
  "model": "flux:latest",
  "timestamp": "2025-02-05T..."
}
```

### 3. Listar Modelos

```bash
curl https://bhjjii.onrender.com/api/models
```

### 4. Gerar Imagem (Teste Manual)

```bash
curl -X POST https://bhjjii.onrender.com/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "um gato colorido"}' \
  -o image.png
```

---

## ğŸ› TROUBLESHOOTING

### Problema: "Connection refused"

**Causa:** Ollama ainda estÃ¡ inicializando
**SoluÃ§Ã£o:** Aguarde 30-45 minutos na primeira vez

### Problema: "Ollama not connected"

**Causa:** ServiÃ§o Ollama nÃ£o estÃ¡ rodando
**SoluÃ§Ã£o:** 
1. VÃ¡ em Render â†’ selecione `ollama-server`
2. Verifique os logs
3. Procure por "Puxando modelo"

### Problema: Timeout 504

**Causa:** Render plano starter Ã© lento
**SoluÃ§Ã£o:** Aguarde mais tempo ou upgrade plano

### Problema: ServiÃ§o dorme

**Causa:** Plano gratuito dorme apÃ³s 15 min inativo
**SoluÃ§Ã£o:** Use UptimeRobot (veja UPTIMEROBOT_GUIDE.md)

---

## ğŸ” SEGURANÃ‡A

Seus dados:
- âœ… Armazenados localmente no Render
- âœ… NÃ£o enviados para terceiros
- âœ… Modelos IA em seu servidor

---

## ğŸ“Š CUSTOS

- API Flask: **Gratuito**
- Ollama Server: **Gratuito**
- Storage modelos: **Gratuito**

**Total: R$0,00** ğŸ‰

---

## ğŸ¯ PRÃ“XIMOS PASSOS

1. âœ… Deploy no Render (vocÃª estÃ¡ aqui)
2. ğŸ”” Configurar UptimeRobot (veja guia)
3. ğŸŒ Acessar frontend: https://seu-repo/frontend/index.html
4. ğŸ¨ Gerar primeiras imagens!

---

**Tudo pronto? Bora fazer o deploy! ğŸš€**
